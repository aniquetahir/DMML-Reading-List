# DMML Reading List
## Causal Learning
- <a name="todo"></a> Class-incremental learning: survey and performance evaluation (**arXiv 2020**) [[paper](https://arxiv.org/abs/2010.15277)]
- <a name="todo"></a> A Comprehensive Study of Replay-based Class Incremental Learning Algorithms (**Neural Networks**) [[paper](https://arxiv.org/pdf/2011.01844.pdf)]
- <a name="todo"></a> Continual learning: A comparative study on how
to defy forgetting in classification tasks (**arXiv 2019**) [[paper](https://arxiv.org/pdf/1909.08383.pdf)]
- <a name="todo"></a> Continual Lifelong Learning with Neural Networks: A Review
 (**Neural Networks**) [[paper](https://arxiv.org/abs/1802.07569)]
## Socially Responsible Algorithms
- <a name="todo"></a> Continual Learning for Natural Language Generation in Task-oriented Dialog Systems(**EMNLP, 2020**) [[paper](https://arxiv.org/abs/2010.00910)] 
- <a name="todo"></a> Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks (**NeurIPS2020**) [[paper](https://proceedings.neurips.cc/paper/2020/file/d7488039246a405baf6a7cbc3613a56f-Paper.pdf)] [[code](https://github.com/ZixuanKe/CAT)]
- <a name="todo"></a> Meta-Consolidation for Continual Learning (**NeurIPS2020**) [[paper](https://arxiv.org/abs/2010.00352?context=cs)] 
- <a name="todo"></a> Understanding the Role of Training Regimes in Continual Learning (**NeurIPS2020**) [[paper](https://arxiv.org/pdf/2006.06958.pdf)] 
- <a name="todo"></a> Continual Learning with Node-Importance based Adaptive Group Sparse Regularization (**NeurIPS2020**) [[paper](https://arxiv.org/pdf/2003.13726.pdf)] 
- <a name="todo"></a> Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning (**NeurIPS2020**) [[paper](https://arxiv.org/pdf/2003.05856.pdf)] 


## Misinformation Detection


## Echo Chamber


## Low-resource Learning
 
